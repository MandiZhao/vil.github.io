<!DOCTYPE HTML>
<html>
	<head>
		<title>BusyBot: Learning to Interact, Reason, and Plan in a BusyBoard Environment</title>
		<link rel="icon" type="image/x-icon" href="favicon.ico?v=1"/>
		<meta charset="utf-8" />
		 <meta name="viewport" content="width=1000">
		<link rel="stylesheet" href="assets/css/main.css" />
		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-W89SB0LDFH"></script>
		<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'G-W89SB0LDFH');
		</script>

		<meta property="og:url"           content="https://busybot.cs.columbia.edu/" />
	    <meta property="og:type"          content="website" />
	    <meta property="og:title"         content="BusyBot: Learning to Interact, Reason, and Plan in a BusyBoard Environment" />
	    <meta property="og:description"   content="We propose a toy-inspired relational environment, BusyBoard, and a learning framework, BusyBot, for embodied AI agents to acquire interaction, reasoning, and planning abilities." />

	</head>
	<body id="top">

		<!-- Main -->
			<div id="main" style="padding-bottom:1em; padding-top: 5em; width: 60em; max-width: 70em; margin-left: auto; margin-right: auto;">
					<section id="four">

						<h1 style="text-align: center; color: #4e79a7; font-size: 225%; margin-bottom: 1em;">BusyBot: Learning to Interact, Reason, and Plan <br/> in a BusyBoard Environment</h1>
						
						<span class="image fit" style="max-width: 100%; margin-top: 0.5em; margin-bottom: 1.5em; margin-left: auto; margin-right: auto">
							<img src="images/teaser.jpg" alt="" />
						</span>

						<p>We introduce BusyBoard, a toy-inspired robot learning environment that leverages a diverse set of articulated objects and inter-object functional relations to provide rich visual feedback for robot interactions. We also introduce a learning framework, BusyBot, which allows an agent to jointly acquire three fundamental capabilities (interaction, reasoning, and planning) in an integrated and self-supervised manner. With the rich sensory feedback provided by BusyBoard, BusyBot first learns a policy to efficiently interact with the environment; then with data collected using the policy, BusyBot reasons the inter-object functional relations through a causal discovery network; and finally by combining the learned interaction policy and relation reasoning skill, the agent is able to perform goal-conditioned manipulation tasks. We evaluate BusyBot in both simulated and real-world environments, and validate its generalizability to unseen objects and relations.
						</p>
		
						<hr>
						<h3>Paper</h3>

						<p style="margin-bottom: 1em;">Latest version: <a href="https://arxiv.org/abs/2207.08192">arXiv</a>
						</p>

						<div class="12u$"><a href="/"><span class="image fit" style="border: 1px solid; border-color: #888888;"><img src="images/paper-thumbnail.jpeg" alt="" /></span></a></div>
						
						<!-- Code is available on <a href="/">GitHub</a>. -->
						
						<hr>
						<h3>Team</h3>
						<section>
							<div class="box alt" style="margin-bottom: 1em;">
								<div class="row 50% uniform" style="width: 90%;">
									<div class="2u" style="font-size: 0.7em; line-height: 1.5em; text-align: center;"><a href="https://lzylucy.github.io/"><span class="image fit" style="margin-bottom: 0.5em;"><img src="images/zeyi_thumbnail.jpg" alt="" style="border-radius: 50%;" /></span>Zeyi Liu</a></div>

									<div class="2u" style="font-size: 0.7em; line-height: 1.5em; text-align: center;"><a href="https://zhenjiaxu.com/"><span class="image fit" style="margin-bottom: 0.5em;"><img src="images/zhenjia_thumbnail.jpg" alt="" style="border-radius: 50%;" /></span>Zhenjia Xu</a></div>
									
									<div class="2u" style="font-size: 0.7em; line-height: 1.5em; text-align: center;"><a href="https://www.cs.columbia.edu/~shurans/"><span class="image fit" style="margin-bottom: 0.5em;"><img src="images/shuran_thumbnail.jpg" alt="" style="border-radius: 50%;" /></span>Shuran Song</a></div>
									
								</div>
							</div>
						</section>
						<p>Columbia University in the City of New York</p>

						<hr>
						<h3>BibTeX</h3>
						<pre><code>@misc{liu2022busybot,
	title={BusyBot: Learning to Interact, Reason, and Plan in a BusyBoard Environment}, 
	author={Zeyi Liu and Zhenjia Xu and Shuran Song},
	year={2022},
	eprint={2207.08192},
	archivePrefix={arXiv},
	primaryClass={cs.RO}}</code></pre>

						<hr>
						<div class="row">
							<div class="12u$ 12u$(xsmall)" style="text-align: center;">
								<h3>Technical Summary Video</h3>
								<iframe id="match-video" width="640" height="360" style="margin-bottom: 2em; margin-left: auto; margin-right: auto; display:block;" src="https://www.youtube.com/embed/EJ98xBJZ9ek" frameborder="0" allowfullscreen></iframe>
							</div>
						</div>

						<hr>
						<h3>Environment and Task</h3>
						<!-- <div class="12u$">
							<span class="image fit" style="max-width: 100%; border: 0px solid; border-color: #888888; margin-left: auto; margin-right: auto">
								<img src="images/board.jpg" alt="">
							</span>
						</div>
						<div class="12u$">
							<span class="image fit" style="max-width: 100%; border: 0px solid; border-color: #888888; margin-left: auto; margin-right: auto">
								<img src="images/board_examples.jpg" alt="">
							</span>
						</div> -->
						<div class="12u$">
							<span class="image fit" style="max-width: 100%; border: 0px solid; border-color: #888888; margin-left: auto; margin-right: auto">
								<img src="images/board_with_examples.jpg" alt="">
							</span>
						</div>
						<p>We build a simulation environment called BusyBoard, which resembles the busyboard toy for kids.
						    Each busyboard environment is composed of trigger objects, responder objects and the inter-object functional relations.
							We procedurally generate busyboard environments with diverse appearances, including different board color/texture, object instances, object layout, and the underlying relations.
						   Based on the BusyBoard environment, we train an embodied agent to learn three fundamental capabilities: interaction, reasoning, and planning.
						</p>

						<hr>
						<h3>Real World Experiments</h3>
						<!-- <div class="12u$">
							<span class="image fit" style="max-width: 80%; border: 0px solid; border-color: #888888; margin-left: auto; margin-right: auto">
								<img src="images/real-interact-reason.jpg" alt="">
							</span>
						</div> -->
						
						<!-- <div>
							<span class="image fit" style="max-width: 22%; display: inline-block">
								<img src="images/real-interact.jpg" alt="">
							</span>
							<video id="1x_speed" controls autoplay loop muted style="width: 50%; display: inline-block"><source src="images/videos/real-world-reasoning.mp4" type="video/mp4"></video>
							<span class="image fit" style="max-width: 22%; display: inline-block">
								<img src="images/real-reason.jpg" alt="">
							</span>
						</div> -->
						<div>
							<img style="width: 24%" src="images/real-interact.jpg" alt="">
							<video id="1x_speed" controls autoplay loop muted style="width: 50%; display: inline-block"><source src="images/videos/real-world-reasoning.mp4" type="video/mp4"></video>
							<img style="width: 23.8%" src="images/real-reason.jpg" alt="">
						</div>

						<p>We test the trained model on a real-world busyboard with robot interactions.
						   We show that our model is able to infer the action affordances from real-world visual observations and reason inter-object functional relations through robot interactions.
						   The video shows reasoning result with respect to interaction steps; the right image shows reasoning result on a few more configurations.
						   More details can be found in the supplementary materials.
						</p>

						<hr>
						<h3>Approach Overview</h3>
						<div class="12u$">
							<span class="image fit" style="max-width: 100%; border: 0px solid; border-color: #888888; margin-left: auto; margin-right: auto">
								<img src="images/approach.jpg" alt="">
							</span>
						</div>
						<p>Our approach contains three modules: the <b>interaction module</b> infers a sequence of actions to efficiently interact with a given scene from visual input. 
							the <b>reasoning module</b> infers a functional scene graph (i.e., inference network) and predicts future states (i.e., dynamic network). 
							Finally, the <b>planning module</b> uses the manipulation policy network (learned from multiple boards), inference and dynamics network (extracted from the specific board) to plan actions for the target state.
						</p>

						<hr>
						<h3>Action Inference and Relation Reasoning Results</h3>
						<!-- <div class="row 50% uniform" style="text-align: center">
							<span class="image fit" style="max-width: 30%; margin-left: auto; margin-right: auto; margin-top: 1em;">
								<img src="images/tables/table1.png" alt="">
							</span>
							<span class="image fit" style="max-width: 70%; margin-left: auto; margin-right: auto">
								<img src="images/tables/table2.png" alt="">
							</span>
						</div> -->

						<div class="12u$">
							<span class="image fit" style="max-width: 100%; border: 0px solid; border-color: #888888; margin-left: auto; margin-right: auto">
								<img src="images/sim-interact-reason.jpg" alt="">
							</span>
						</div>
						<p>(a) shows the learned position and direction affordances; (b) shows interaction steps and corresponding reasoning results; (c) shows reasoning results on more configurations. The red arrow indicates inferred inter-object functional relations and the green arrow indicates ground truth.</p>
						

						<hr>
						<h3>Goal-conditioned Manipulation Results</h3>
						<div class="12u$">
							<span class="image fit" style="max-width: 80%; border: 0px solid; border-color: #888888; margin-left: auto; margin-right: auto">
								<img src="images/sim-goal-conditioned.jpg" alt="">
							</span>
						</div>
						<p>Compared to the predictive agent which relies on correct future state predictions from the dynamics model, the relation agent generalizes better on novel objects. 
							However, the relation agent struggles in handling one-to-many relations since it cannot infer the exact action position and direction. 
							Our method(BusyBot) combines the advantages of both agents.
						</p>

						<!-- <div class="12u$">
							<span class="image fit" style="max-width: 80%; border: 0px solid; border-color: #888888; margin-left: auto; margin-right: auto">
								<img src="images/sim-novel-config-goal-conditioned.jpg" alt="">
							</span>
						</div>
						<p>More results on busyboards with novel configurations and seen object instances. The predictive agent achieves better performance on one-to-many tasks with seen object instances by leveraging future predictions to select the correct action to apply on the trigger object. In contrast, the relation agent can only identify the trigger object but not the exact action (e.g., which link to interact with or which direction to push).
						</p>
						<div class="12u$">
							<span class="image fit" style="max-width: 80%; border: 0px solid; border-color: #888888; margin-left: auto; margin-right: auto">
								<img src="images/sim-novel-object-goal-conditioned.jpg" alt="">
							</span>
						</div>
						<p>More results on busyboards with novel configurations and novel object instances. The relation agent performs slightly better than the predictive agent on boards with novel objects, when the dynamics model fails to predict the correct next state. This shows that inter-object functional relationship can generalize to scenarios when future predictions are not reliable enough to assist planning. -->

						<!-- <hr>
				        <h3>Acknowledgements</h3>
				        <p> This work was supported by National Science Foundation under CMMI-2037101 and Amazon Research Award. We would like to thank Google for the UR5 robot hardware.</p>
						<hr> -->

						<hr>
						<h3>Acknowledgements</h3>
						<p>This work was supported in part by National Science Foundation under 2143601, 2037101, and 2132519. 
						   Many thanks to Huy Ha, Cheng Chi, Samir Gadre, Neil Nie, and Zhanpeng He on valuable feedback and support.
						   We would also like to thank Google for the UR5 robot hardware.
						   The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the sponsors.
						</p>
						
						<h3>Contact</h3>
				        <p>If you have any questions, please feel free to contact <a href="https://lzylucy.github.io/">Zeyi Liu</a>.</p>
						
					</section>

					
			</div>

			<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.poptrox.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>
	</body>
</html>
